1.大模型三种模式：(1) Embedded (2) Copilot (3) Agent

2.技术架构：(1) 纯prompt (2) Agent + Functional Calling

3.单Agent和Multi-Agent

4.RAG与Fine-tuning

[[Link]](https://www.xiaohongshu.com/discovery/item/697de305000000001a021ad9?source=webshare&xhsshare=pc_web&xsec_token=ABIkHP3dGmjy5pjpMKV0UIJM9K-Xdlkz-Eoi0SVBagCoU=&xsec_source=pc_share)

1.Transformer，Mixture of Experts (MOE)，Ensemble Learning

[[Link]](https://www.xiaohongshu.com/explore/687267bb0000000024008f32?xsec_token=ABSAeMWFhhjze_1BMsTHKBs-S-gaL7QR6KZmfpxhp8RQ4=&xsec_source=pc_search&source=web_search_result_notes)

Multimodal Large Language Models:

1.LLaVA [24] map image tokens to pre-trained LLM representations via a learned projector.

2.BLIP-2 [18, 19] employs a query transformer to generate image embeddings from learned queries after extracting image features. 

3.MoVA [65] uses an adaptive router to integrate task-specific vision experts with a coarse-to-fine approach. 

4.Qwen2-VL [45] enhances high-resolution image understanding through dynamic resolution handling and flexible tokenization 

Visual Search:

Early (trajectories):

1.Bayesian frameworks with saliency maps [38, 42]

2.deep networks for similarity mapping [60]

3.inverse reinforcement learning (IRL)[53] 

Recently:

1.SEAL [51] integrates **localization modules** and visual memory to combine visual search with large multimodal models (LMMs) to guide attention regions, but it requires additional training for effective visual search. 

Visual Experts (SAM-based text-prompted segmentation):

1.Grounded-SAM combines text-prompted boxes generated by Grounding DINO with SAM’s segmentation function to create a two-stage framework [25, 35]. 

2.Fast-SAM matches text and image regions of interest based on CLIP feature similarity [32, 63]. 

3.RefSAM uses a cross-modal MLP to project text embeddings into SAM’s sparse and dense embeddings.

4.LISA leverages large multimodal models like LLaVA to extract multimodal embeddings for SAM [21, 24]. 

[[Link]](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs_CVPR_2025_paper.pdf)